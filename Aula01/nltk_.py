import nltk
from nltk.corpus import stopwords, treebank
from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer, LancasterStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag, pos_tag_sents
import string
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('tagsets')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

texto = "Nós somos feitos de poeiras de estrelas. Nós somos uma maneira de os cosmos se auto conhecer. A imaginação nos leva a mundos que sequer existiram, mas sem ela não vamos a lugar algum."

sentencas = sent_tokenize(texto, language="portuguese")

print(sentencas)
print(type(sentencas))
print(len(sentencas))

words = word_tokenize(texto,language="portuguese")

print(words)
print(type(words))
print(len(words))

stops = stopwords.words("portuguese")
print(len(stops))
print(stops)

words_wo_stopwords = [w for w in words if w not in stops]

print(words_wo_stopwords)